# LoRA Mint

**Train it once. Style it forever.** Prompt, mint, and create with your own visual signature.

A full-stack web application that lets users:
- Generate images from text prompts using Stable Diffusion
- Upload reference images to train custom LoRA adapters
- Apply trained LoRAs to future image generations
- View and manage image outputs and LoRA files

---

## ğŸ§± Tech Stack

| Layer     | Technology                |
|-----------|---------------------------|
| Frontend  | Blazor Server (C#)         |
| Backend   | ASP.NET Core Minimal APIs  |
| AI Engine | Python (FastAPI)           |
| Models    | SDXL Base, SDXL Turbo, Z-Image Turbo |
| LoRA      | PEFT / Kohya Trainer       |
| Format    | `.safetensors`             |
| Storage   | Local File System          |

---

## ğŸš€ How It Works

1. **Setup**: First-time users are guided through a setup wizard to select and download a model
2. **Generate**: User enters a prompt â†’ Python backend generates image using the selected model
3. **Train**: User uploads 1-5 reference images â†’ LoRA model is trained and saved as `.safetensors`
4. **Apply**: User selects trained LoRA(s) â†’ Generates stylized images with custom style
5. **Manage**: All LoRAs and images are organized per user for easy access

---

## ğŸ¤– Supported Models

| Model | Speed | Quality | Min VRAM | LoRA Support |
|-------|-------|---------|----------|--------------|
| **SDXL Base 1.0** | Medium (30 steps) | High | 8GB | Yes |
| **SDXL Turbo** | Fast (4 steps) | Good | 8GB | Yes |
| **Z-Image Turbo** | Fast (8 steps) | Excellent | 16GB | No |

Models are downloaded on-demand through the setup wizard or settings page. Each model is ~7-12GB.

---

## ğŸ“ Project Structure

```
LoraMint/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ LoraMint.Web/              # Blazor Server application
â”‚   â”‚   â”œâ”€â”€ Components/            # Blazor components
â”‚   â”‚   â”‚   â””â”€â”€ Shared/            # Reusable components (ModelComparisonTable)
â”‚   â”‚   â”œâ”€â”€ Models/                # Data models (ModelConfig, etc.)
â”‚   â”‚   â”œâ”€â”€ Pages/                 # Razor pages (Setup, Settings, Generate, etc.)
â”‚   â”‚   â”œâ”€â”€ Services/              # C# services (ModelConfigurationService)
â”‚   â”‚   â””â”€â”€ Program.cs             # Minimal API endpoints
â”‚   â”‚
â”‚   â””â”€â”€ python-backend/            # Python FastAPI backend
â”‚       â”œâ”€â”€ models/                # Pydantic models
â”‚       â”œâ”€â”€ services/              # AI services
â”‚       â”‚   â”œâ”€â”€ image_generator.py # SD image generation
â”‚       â”‚   â”œâ”€â”€ model_manager.py   # Model downloading/loading
â”‚       â”‚   â””â”€â”€ lora_trainer.py    # LoRA training
â”‚       â”œâ”€â”€ utils/                 # Utilities
â”‚       â””â”€â”€ main.py                # FastAPI application
â”‚
â”œâ”€â”€ data/                          # Storage (gitignored)
â”‚   â”œâ”€â”€ models/                    # Downloaded AI models
â”‚   â”œâ”€â”€ loras/                     # User LoRA models
â”‚   â”œâ”€â”€ outputs/                   # Generated images
â”‚   â””â”€â”€ model-settings.json        # User preferences
â”‚
â”œâ”€â”€ PROJECT_INSTRUCTIONS.md        # Detailed specifications
â”œâ”€â”€ docker-compose.yml             # Docker orchestration
â””â”€â”€ README.md                      # This file
```

---

## ğŸ”„ Local Development Setup

### Prerequisites

- .NET 8.0 SDK
- Python 3.10+
- CUDA-capable GPU (recommended)
- 16GB+ RAM

### Quick Start (Recommended)

The Blazor application **automatically sets up and starts the Python backend** for you!

**Linux/macOS:**
```bash
./start.sh
```

**Windows:**
```cmd
start.bat
```

That's it! The application will:
1. Stop any existing LoraMint instances (automatic cleanup)
2. Create Python virtual environment (if needed)
3. Install dependencies (if needed)
4. Start the Python backend
5. Start the Blazor web application

Access the app at `https://localhost:5001`

ğŸ“– **See [QUICKSTART.md](QUICKSTART.md) for detailed first-run instructions**

### Manual Setup (Optional)

If you prefer manual control:

#### 1. Setup Python Environment

**Linux/macOS:**
```bash
./setup-python.sh
```

**Windows:**
```cmd
setup-python.bat
```

Or manually:
```bash
cd src/python-backend
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

#### 2. Start Applications

**Option A:** Use startup script (auto-starts Python backend)
```bash
./start.sh  # or start.bat on Windows
```

**Option B:** Start manually in separate terminals

Terminal 1 - Python Backend:
```bash
cd src/python-backend
source venv/bin/activate  # On Windows: venv\Scripts\activate
python main.py
```

Terminal 2 - Blazor Web:
```bash
cd src/LoraMint.Web
dotnet run
```

#### 3. Access the Application

Open your browser and navigate to:
- Web UI: `https://localhost:5001`
- Python API Docs: `http://localhost:8000/docs`

---

## ğŸ³ Docker Deployment

### Using Docker Compose

```bash
# Build and run all services
docker-compose up --build

# Run in detached mode
docker-compose up -d

# View logs
docker-compose logs -f

# Stop services
docker-compose down
```

Services will be available at:
- Web UI: `http://localhost:5001`
- Python API: `http://localhost:8000`

---

## ğŸ“– API Documentation

### Blazor Minimal APIs

- `POST /api/generate` - Generate an image from a prompt
- `POST /api/train-lora` - Train a new LoRA model
- `GET /api/loras/{userId}` - List user's LoRA models
- `GET /api/images/{userId}` - List user's generated images

### Python FastAPI Endpoints

- `POST /generate` - Generate image using Stable Diffusion
- `POST /train-lora` - Train LoRA model
- `GET /loras/{user_id}` - Get user's LoRAs
- `GET /images/{user_id}` - Get user's images
- `GET /health` - Health check and GPU status
- `GET /models` - List available models with download status
- `POST /models/{id}/download` - Download a model (SSE progress stream)
- `POST /models/{id}/load` - Load model into GPU memory
- `POST /models/unload` - Unload current model
- `GET /models/current` - Get currently loaded model
- `GET /system/gpu` - Get GPU information (VRAM, CUDA version)

For detailed API documentation, visit `http://localhost:8000/docs` after starting the Python backend.

---

## ğŸ¨ Features

### Model Selection & Setup
- First-time setup wizard with GPU detection
- Model comparison table with VRAM requirements
- Color-coded compatibility indicators (green/yellow/red based on VRAM)
- On-demand model downloading with progress streaming
- Dynamic "Powered by [Model Name]" label
- Settings page for model management

### Cyberpunk Terminal Theme
- Dark theme with gradient mesh background
- Purple/pink/orange gradient accents with cyan highlights
- Terminal-style typography (JetBrains Mono)
- Animated loading states with pulsing dots and spinning rings
- Glowing UI elements and scanline effects

### Image Generation
- Text-to-image using SDXL Base, SDXL Turbo, or Z-Image Turbo
- Optional LoRA model application
- Multiple LoRAs with adjustable strength
- Real-time generation feedback with animated progress
- Step counter with visual loading animations

### LoRA Training
- Upload 1-5 reference images
- Custom naming for organization
- Automatic .safetensors format
- Per-user storage

### Gallery Management
- Browse all generated images
- View generation metadata
- Filter by date
- Download images

### LoRA Library
- List all trained models
- View file information
- Quick access for generation
- Delete unwanted models

---

## ğŸ”§ Configuration

### Blazor Web (`src/LoraMint.Web/appsettings.json`)

```json
{
  "PythonBackend": {
    "BaseUrl": "http://localhost:8000",
    "Path": "../python-backend",
    "AutoStart": true,
    "AutoInstallDependencies": true
  },
  "Storage": {
    "LorasPath": "../../data/loras",
    "OutputsPath": "../../data/outputs"
  }
}
```

**Configuration Options:**
- `AutoStart`: Automatically start Python backend (default: `true`)
- `AutoInstallDependencies`: Auto-install Python packages (default: `true`)
- `Path`: Path to Python backend directory
- `BaseUrl`: Python backend API URL

### Python Backend (`src/python-backend/main.py`)

- Model selection (SDXL, SD Turbo, etc.)
- Generation parameters
- Storage paths
- GPU settings

---

## ğŸš§ Development Status

### âœ… Implemented
- Blazor Server UI with all pages
- ASP.NET Core Minimal APIs
- FastAPI backend structure
- Image generation pipeline with real-time SSE progress streaming
- File storage system
- Docker support
- **Automatic Python backend startup**
- **One-command setup and launch**
- **Enhanced startup feedback with progress indicators**
- Cross-platform startup scripts (Windows & Linux/macOS)
- **Automated dependency installation and validation**
- **Real LoRA training using DreamBooth-style PEFT** (see Known Issues)
- **Training UI with progress streaming and configurable settings**
- **Fast mode for quicker training (~40% faster)**
- **Cyberpunk terminal dark theme with gradient accents**
- **Animated loading states (pulsing dots, spinning rings)**
- **Multi-model selection (SDXL Base, SDXL Turbo, Z-Image Turbo)**
- **Setup wizard for first-time users with GPU detection**
- **Model comparison table with VRAM compatibility indicators**
- **Settings page for model management**
- **Network-friendly model downloads (throttled to prevent saturation)**

### ğŸ”¨ In Progress
- LoRA training memory optimization for 10GB GPUs
- User authentication
- Image metadata persistence

### ğŸ“‹ Planned
- LoRA stacking UI with sliders
- Azure Blob Storage support
- Batch generation
- LoRA marketplace
- Additional model support (SDXL Lightning, Playground v2.5, etc.)

---

## âš ï¸ Known Issues

### LoRA Training Memory (10GB VRAM GPUs)

The current DreamBooth-style LoRA training implementation may experience out-of-memory (OOM) issues on GPUs with 10GB VRAM (e.g., RTX 3080). This occurs because:

1. **Class image generation** loads a full SDXL pipeline (~8GB)
2. **Training** loads UNet, VAE, and text encoders
3. CUDA memory fragmentation prevents efficient reuse

**Current mitigations implemented:**
- Text encoders run on CPU (FP32) instead of GPU
- Aggressive GPU memory cleanup between phases
- Gradient checkpointing enabled
- 8-bit Adam optimizer
- Mixed precision (FP16) training

**Workarounds for users:**
- Use GPUs with 12GB+ VRAM for reliable training
- Close other GPU-intensive applications during training
- If training fails, restart the app and try again (class images are cached)

**Future fixes planned:**
- Sequential model loading with offloading
- Lower resolution option for class image generation
- Memory-efficient attention (xFormers) when available

---

## ğŸ“š Additional Documentation

- [Project Instructions](PROJECT_INSTRUCTIONS.md) - Detailed specifications
- [Python Backend README](src/python-backend/README.md) - Python setup guide
- [Blazor Web README](src/LoraMint.Web/README.md) - .NET setup guide

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- Stable Diffusion by Stability AI
- diffusers library by Hugging Face
- PEFT for LoRA implementation
- Blazor framework by Microsoft

---

## ğŸ“ Support

For issues and questions:
- Open an issue on GitHub
- Check existing documentation
- Review API documentation at `/docs`
